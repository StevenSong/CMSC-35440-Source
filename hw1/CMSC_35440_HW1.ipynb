{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "vfHBZQl0rauN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CMSC 35440 Machine Learning in Biology and Medicine\n",
        "## Homework 1: Embedding Research Articles\n",
        "**Released**: Jan 14, 2025\n",
        "\n",
        "**Due**: Jan 24, 2025 at 11:59 PM Chicago Time on Gradescope\n",
        "\n",
        "**In this first homework, you'll generate embeddings for 20 provided research articles and visualize them.**\n",
        "\n",
        "At a high-level, embeddings are feature-vectors computed by some model that aim to extract information from the underlying sample. In 2025, many folks instinctively turn to massive deep neural nets as these feature-extractors, however there is nearly a century of literature on other feature-extraction methods that are still very much relevant!\n",
        "\n",
        "For this homework, you'll use term frequency-inverse document frequency (tf-idf) as the feature extraction method. This method dates back to 1972 so it's over 50 years old! Through this homework, hopefully we'll convince you that it's still very much relevant.\n",
        "\n",
        "Please carefully read through the instructions below. Also, while not required for the homework, the articles themselves are worth a read! They're some seminal papers across various domains, including biomedical AI/ML."
      ],
      "metadata": {
        "id": "K-gV2pmJcU79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "vfHBZQl0rauN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Download and open the starter notebook up in your favorite Jupyter Notebook host. We recommend using [Google Colab](https://colab.research.google.com/).\n",
        "  * **NB:** We'll design all homeworks such that they can be run on the *free* tier of Colab. You're welcome to use any other host, but the benefit of Colab is that they offer free GPU-instances.\n",
        "  * Technically you don't need GPUs for any modeling but it really does speed it up! For homeworks where GPU-acceleration is recommended, we'll provide additional instructions on how to access GPU-instances on Colab.\n",
        "  * For this homework, we don't require the use of any GPUs, though it may help if you're interested in the for-your-benefit extension.\n",
        "1. Download and unzip the research articles. We've provided them as a tarball that be downloaded from [https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz](https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz).\n",
        "  * You'll notice that there's a CSV of article metadata and a folder of article *PDFs*. These articles are technically available as extracted-text (come ask in office-hours if you're interested in using such a research for other projects), but real-world data is messy. One such way that data can be mesy is that it only exists as PDFs - so **you must use the article PDFs for this assignment**.\n",
        "1. Extract the text from the articles. Use variables from the metadata as you see fit.\n",
        "1. Compute TF-IDF matrix for the corpus. **You must implement TF-IDF yourself. You may not use any existing implementations for computing the TF-IDF matrix** (e.g. you can NOT use sklearn's function for TF-IDF).\n",
        "  * Defining what is a \"term\" is up to you but don't overcomplicate it. Splitting on whitespace characters works fine.\n",
        "  * The wikipedia is hopefully all you need to understand the formula: [https://en.wikipedia.org/wiki/Tf-idf](https://en.wikipedia.org/wiki/Tf-idf).\n",
        "1. Normalize your embeddings. Normalization is an important step to make embeddings comparable between samples.\n",
        "1. Visualize your embeddings. Embeddings are typically used in some downstream application, but visualization at this stage can be a nice sanity check before proceeding with further modeling - have your embeddings actually captured information that reflect the underlying data?\n",
        "  * Your embeddings are probably high-dimensional vectors. Humans have a hard time visualizing things beyond 3 dimensions and honestly we can get away with 2 dimensions in most cases.\n",
        "  * There are many methods to do unsupervised dimensionality reduction. Some of the classical methods include principal component analysis (PCA), uniform manifold approximation and projection (UMAP), and t-distributed stochastic neighbor embedding (t-SNE). These are all fine methods for this homework as they are provided by existing packages.\n",
        "  * However, beware of the pitfalls of methods such as UMAP and t-SNE which are highly succeptible to the hyperparameters used with the underlying data. This is a nice post detailing these pitfalls, check out the mammoth figure: [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/).\n",
        "  * In your visualization plot, it may be helpful to incorporate aspects of the metadata. We'll leave that open ended; visualize in a way that you think will help your discussion.\n",
        "1. After you're happy with your work, analyze your results, writeup what you've done, and submit the homework to Gradescope.\n",
        "  * Your submission should include 2 things:\n",
        "    1. Your writeup containing a figure with your embedding visualization.\n",
        "    1. Your notebook with your code for computing TF-IDF and generating the figure.\n",
        "  * Your writeup should be 0.5 to 1 page long. This length should be *before* including your figure. The text should be size 12pt, single spaced, with 1 inch margins, and on letter size paper. Please submit either a PDF or Word document.\n",
        "  * Some guiding questions: Have your embeddings actually captured underlying information about the articles? How can you tell? Why are some articles embedded closer to each other while others are not?\n",
        "\n",
        "**Tips and Tricks:**\n",
        "1. In general, you're welcome to use any tools you need for this homework. The only exceptions have been noted in the instructions.\n",
        "1. Reading CSVs can be done with `pandas`. We'll use `pandas` plenty more in the future so be sure to familiarize yourself with it.\n",
        "1. Extracting text from PDFs is relatively simple these days with [`pypdf`](https://github.com/py-pdf/pypdf).\n",
        "    * If you've used PyPDF2 in the past, that package has been merged back into and development has resumed on the original pypdf project. So make the switch back! This change was made around the end of 2022. You can see the release notes [here](https://github.com/py-pdf/pypdf/releases/tag/3.1.0).\n",
        "1. `numpy` will probably be useful in the normalization step.\n",
        "1. You should probably use `matplotlib` or derivative (e.g. `seaborn`) for visualization.\n",
        "1. If you're looking for guidance on any part of the homework or related topics, email Steven (songs1@uchicago.edu) or come to office hours! JCL 205 Wed 11a - 12p. Also open to scheduling 1:1 meetings if this time does not work for you, just email to ask."
      ],
      "metadata": {
        "id": "qW_R-TgIriQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "w4aMfZ0krmzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!wget https://github.com/StevenSong/CMSC-35440-Source/releases/download/hw1/hw1.tar.gz\n",
        "!tar -xzf hw1.tar.gz"
      ],
      "metadata": {
        "id": "RLCvyGgskxHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6kMkUasW6Aw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}